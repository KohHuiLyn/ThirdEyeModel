{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Final Year Project - The Third Eye </h1>\n",
    "Done by: See Zhuo Yi Joey (2011927), Liu Zhen (2021250), Koh Hui Lyn (2021672) and Ang Jun Hoa (2040295)\n",
    "<br/>Project Aim: Using computer vision to aid coach’s analysis of a bowler’s performance by producing consistent and accurate intelligent analysis.\n",
    "<br/>Modules Required: MediaPipe, OpenCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Installing Required Packages</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install mediapipe\n",
    "# pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Importing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing OpenCV to help us process (read/write) videos\n",
    "import cv2\n",
    "\n",
    "# Import Math to help us with some calculations\n",
    "import math as m\n",
    "\n",
    "# Import pandas to help us with storing of previous frames' information\n",
    "import pandas as pd\n",
    "\n",
    "# Import MediaPipe to help us with Pose Estimation\n",
    "import mediapipe as mp\n",
    "\n",
    "# Import some utils modules\n",
    "import os\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Functions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Distance between 2 points using Pythagoras Theorem\n",
    "def findDistance(x1, y1, x2, y2):\n",
    "    dist = m.sqrt((x2-x1)**2+(y2-y1)**2)\n",
    "    return dist\n",
    "\n",
    "# Calculate angle between 2 points\n",
    "def findAngle(x1, y1, x2, y2):\n",
    "    theta = m.acos((y2 -y1)*(-y1) / (m.sqrt((x2 - x1)**2 + (y2 - y1)**2) * y1))\n",
    "    degree = int(180/m.pi)*theta\n",
    "    return degree\n",
    "\n",
    "# Calculate difference of X-Coordinate of two points\n",
    "def findX(x_knee,x_hand):\n",
    "  X = x_hand - x_knee\n",
    "  return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Utils</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Font (For OpenCV Video)\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "# Preset Colors for easy calling\n",
    "blue = (255, 127, 0)\n",
    "red = (50, 50, 255)\n",
    "green = (127, 255, 0)\n",
    "dark_blue = (127, 20, 0)\n",
    "light_green = (127, 233, 100)\n",
    "yellow = (0, 255, 255)\n",
    "pink = (255, 0, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From all the Mediapipe Computer Vision Solutions, select to use Mediapipe Pose============================================\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# Call the MediaPipe Pose Model with defined parameters.\n",
    "# min_detection_confidence - minimum confidence required to detect a PERSON (not landmarks)\n",
    "# min_tracking_confidence  - minimum confidence required to detect the landmarks\n",
    "# model_complexity - complexity of the pose landmark model (0,1,2) Where 2 is the most complex, increasing landmark accuracy and time taken to run\n",
    "# smooth_landmarks - reduce the jitter for the detected landmarks based on the previous landmark position\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.2, min_tracking_confidence=0.2, model_complexity=2, smooth_landmarks=True)\n",
    "\n",
    "\n",
    "# Choose which video to use=================================================================================================\n",
    "file_name = './videos/23JUL/IMG_7657.MOV'\n",
    "cap = cv2.VideoCapture(file_name)\n",
    "\n",
    "# CV2  properties===========================================================================================================\n",
    "\n",
    "# Get the FPS, Width, and Height of the Video\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Getting the frame size ie 1920 x 1080\n",
    "frame_size = (width, height)\n",
    "\n",
    "# Video Codec to help store and playback the video (required for the VideoWriter)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "# Initialize video writer with the output filename, fourcc, fps of the vid and frame size\n",
    "video_output = cv2.VideoWriter(\"IMG_7657.mp4\", fourcc, fps, frame_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Main Code</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the dataframes\n",
    "\n",
    "# Dataframe to store the Velocity Information\n",
    "feetVelo = pd.DataFrame(columns=[\"Frame\",\"LH_X\",\"LI_X\", \"Heel Velo\", \"Index Velo\"])\n",
    "\n",
    "# Dataframe to get the average FeetSize\n",
    "feetSizeInfo = pd.DataFrame(columns=['Frame', 'AvgLen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting...\n",
      "trying\n",
      "bigger than 1080,  1920\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "trying\n",
      "No frames left to process!!!\n",
      "Video is done!\n"
     ]
    }
   ],
   "source": [
    "# Start of the program=======================================================================\n",
    "\n",
    "print('Starting...')\n",
    "\n",
    "# Variables for dynamic font size\n",
    "fontsize = 1\n",
    "thick = 4\n",
    "text_y = 50\n",
    "font_access = 1\n",
    "\n",
    "# Variables for Step Counter\n",
    "steps = 0\n",
    "stage = None\n",
    "maxFeetLength = 50\n",
    "\n",
    "# Variables for Timing\n",
    "access = 1\n",
    "velo = 0\n",
    "ball_release = None\n",
    "sliding = False\n",
    "\n",
    "currentFrame= 0\n",
    "currentframenumber=dict.fromkeys([\"Angle1\",\"Angle2\",\"Angle3\",\"Angle4\",\"Angle5\",\"Release\"])\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "while cap.isOpened():\n",
    "    # Capture frames\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "        print(\"No frames left to process!!!\")\n",
    "        break\n",
    "    # Get fps, height and width\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    h, w = image.shape[:2]\n",
    "    # Convert the BGR image to RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # Process the frame with Mediapipe Pose\n",
    "    keypoints = pose.process(image)\n",
    "    # Convert the image back to BGR\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    # Get the current frame number\n",
    "    currentFrame = cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "\n",
    "    \n",
    "    #============ Getting landmarks ============\n",
    "    lm = keypoints.pose_landmarks\n",
    "    lmPose = mp_pose.PoseLandmark\n",
    "    try:\n",
    "        print(\"trying\")\n",
    "        # To account to most videos being rectangular, we have to multiply width and height\n",
    "        # X-Axis will multiply be the width of the video and the Y-Axis will multiply by the height\n",
    "        # This is to remain the aspect ratio of the video, and make sure the X and Y will be in the same scale.\n",
    "\n",
    "        # For Angle Component \n",
    "        l_shldr_x = int(lm.landmark[lmPose.LEFT_SHOULDER].x * w)\n",
    "        l_shldr_y = int(lm.landmark[lmPose.LEFT_SHOULDER].y * h)\n",
    "        r_shldr_x = int(lm.landmark[lmPose.RIGHT_SHOULDER].x * w)\n",
    "        r_shldr_y = int(lm.landmark[lmPose.RIGHT_SHOULDER].y * h)\n",
    "        l_hip_x = int(lm.landmark[lmPose.LEFT_HIP].x * w)\n",
    "        l_hip_y = int(lm.landmark[lmPose.LEFT_HIP].y * h)\n",
    "\n",
    "        # Ankles for feet distance calculation\n",
    "        l_ank_x = int(lm.landmark[lmPose.LEFT_ANKLE].x * w)\n",
    "        l_ank_y = int(lm.landmark[lmPose.LEFT_ANKLE].y * h)\n",
    "        r_ank_x = int(lm.landmark[lmPose.RIGHT_ANKLE].x * w)\n",
    "        r_ank_y = int(lm.landmark[lmPose.RIGHT_ANKLE].y * h)\n",
    "\n",
    "        r_ind_x = int(lm.landmark[lmPose.RIGHT_INDEX].x * w)\n",
    "        r_ind_y = int(lm.landmark[lmPose.RIGHT_INDEX].y * h)\n",
    "        r_heel_x = int(lm.landmark[lmPose.RIGHT_HEEL].x * w)\n",
    "        r_heel_y = int(lm.landmark[lmPose.RIGHT_HEEL].y * h)\n",
    "\n",
    "        l_ind_x = int(lm.landmark[lmPose.LEFT_INDEX].x * w)\n",
    "        l_ind_y = int(lm.landmark[lmPose.LEFT_INDEX].y * h)\n",
    "        l_heel_x = int(lm.landmark[lmPose.LEFT_HEEL].x * w)\n",
    "        l_heel_y = int(lm.landmark[lmPose.LEFT_HEEL].y * h)\n",
    "\n",
    "        # For Timing Component\n",
    "        r_wrist_x = int(lm.landmark[lmPose.RIGHT_WRIST].x * w)\n",
    "        r_knee_x = int(lm.landmark[lmPose.RIGHT_KNEE].x * w)\n",
    "        l_knee_x = int(lm.landmark[lmPose.LEFT_KNEE].x * w)\n",
    "\n",
    "        # When x-val is at the max, foot is placed on the floor\n",
    "        if (currentFrame < 20):\n",
    "            feetLength = r_ind_x - r_heel_x\n",
    "            if feetLength > maxFeetLength:\n",
    "                maxFeetLength = feetLength\n",
    "\n",
    "\n",
    "\n",
    "        #============ Functions ============\n",
    "        # feetDist = findDistance(l_ank_x, l_ank_y, r_ank_x, r_ank_y)\n",
    "        feetDist = abs(findX(l_ank_x, r_ank_x))\n",
    "\n",
    "        # Steps Counter (To be improved - ie Thresholds improvements)\n",
    "        if steps < 5:\n",
    "            if feetDist > maxFeetLength and stage == 'up':\n",
    "                steps += 1\n",
    "                stage = \"down\"\n",
    "            elif feetDist < maxFeetLength:\n",
    "                stage = \"up\"\n",
    "        # Timing Component\n",
    "        # When feet dist is big, then it means it's sliding. Sooooooo enter this\n",
    "        if (feetDist > 3*maxFeetLength and steps ==5)or sliding==True:\n",
    "            sliding=True\n",
    "            # currentFrame = cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "            pre = currentFrame - 2\n",
    "            if currentFrame > 10:\n",
    "                # Calculate Velocity with this frame and 4 frames before\n",
    "                velo = abs(((l_heel_x - feetVelo[\"LH_X\"][pre])/(currentFrame-pre)))\n",
    "                if velo == 0 and access == 1:\n",
    "                    access = 0\n",
    "                    ball_train_feet_dis = findX(r_knee_x, r_wrist_x)\n",
    "                    ball_slide_feet_dis = findX(l_knee_x, r_wrist_x)\n",
    "                    if ball_train_feet_dis < 0:\n",
    "                        ball_release = \"Late\"\n",
    "                    elif ball_train_feet_dis > 0 and ball_slide_feet_dis < 0:\n",
    "                        ball_release = \"Traditional\"\n",
    "                    elif ball_slide_feet_dis > 0:\n",
    "                        ball_release = \"Early\"\n",
    "                    currentframenumber['Release']=[currentFrame+1]\n",
    "    \n",
    "        # Append to array\n",
    "        feetStuff = {\"Frame\": currentFrame+1, \"LH_X\":l_heel_x,\"LI_X\":l_ind_x,\"Velocity\": velo}\n",
    "        feetVelo = feetVelo.append(feetStuff, ignore_index=True)\n",
    "\n",
    "        # Calculate torso and neck angles\n",
    "        torso_inclination = findAngle(l_hip_x, l_hip_y, l_shldr_x, l_shldr_y)\n",
    "\n",
    "        #============ Annotations onto video ============\n",
    "        # Define font size \n",
    "        if h >= 2160 and font_access == 1:\n",
    "            print(\"bigger than 2160\", w)\n",
    "            thick = 10\n",
    "            text_y = 80\n",
    "            fontsize = 3\n",
    "            font_access = 0\n",
    "        elif h >= 1080 and font_access == 1:\n",
    "            print(\"bigger than 1080, \", w)\n",
    "            fontsize = 1.5\n",
    "            font_access = 0\n",
    "        elif font_access == 1:\n",
    "            print(\"smaller than 1080, \", w)\n",
    "            thick = 2\n",
    "            fontsize = 0.6\n",
    "            font_access = 0\n",
    "\n",
    "        # # BACK ANGLE\n",
    "        # # Check for Camera Alignment to be in Proper Sideview\n",
    "        # offset = findDistance(l_shldr_x, l_shldr_y, r_shldr_x, r_shldr_y)\n",
    "        # if offset < 100:\n",
    "        #     cv2.putText(image, str(int(offset)) + ' Aligned', (10, text_y), font, fontsize, green, thick)\n",
    "        # else:\n",
    "        #     cv2.putText(image, str(int(offset)) + ' Not Aligned', (10 , text_y), font, fontsize, red, thick)\n",
    "        # # Draw landmarks\n",
    "        # cv2.circle(image, (l_shldr_x, l_shldr_y), 7, yellow, -1)\n",
    "        # cv2.circle(image, (l_shldr_x, l_shldr_y - 100), 7, yellow, -1)\n",
    "        # # Right shoulder is pink ball\n",
    "        # cv2.circle(image, (r_shldr_x, r_shldr_y), 7, pink, -1)\n",
    "        # cv2.circle(image, (l_hip_x, l_hip_y), 7, yellow, -1)\n",
    "        # cv2.circle(image, (l_hip_x, l_hip_y - 100), 7, yellow, -1)\n",
    "        # # STR for back angle\n",
    "        # angle_text_string ='Frame: '+str(currentFrame)+' Torso Angle : ' + str(int(torso_inclination)) + 'deg Feet distance: '+ str(int(feetDist)) + ' Steps: '+ str(int(steps)) \n",
    "        # cv2.putText(image, angle_text_string, (10, text_y), font, fontsize, dark_blue, thick, cv2.LINE_AA)\n",
    "        # # Join landmarks\n",
    "        # cv2.line(image, (l_shldr_x, l_shldr_y), (l_shldr_x, l_shldr_y - 100), green, thick)\n",
    "        # cv2.line(image, (l_hip_x, l_hip_y), (l_shldr_x, l_shldr_y), green, thick)\n",
    "        # cv2.line(image, (l_hip_x, l_hip_y), (l_hip_x, l_hip_y - 100), green, thick)\n",
    "        # # Display angles on the annotation\n",
    "        # cv2.putText(image, str(int(torso_inclination)), (l_hip_x + 10, l_hip_y), font, fontsize, pink, thick, cv2.LINE_AA)\n",
    "\n",
    "        # TIMING\n",
    "        # Display the skeleton\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            keypoints.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "        # Text for Neck/Torso Angle, Feet distance & Steps\n",
    "\n",
    "        # STR for timing\n",
    "        angle_text_string = 'Frame: '+str(currentFrame) +' Feet distance: '+ str(int(feetDist)) + ' Steps: '+ str(int(steps))  + ' Release: '+ str(ball_release) + ' Velocity '+ str(velo)\n",
    "        cv2.putText(image, angle_text_string, (10, text_y), font, fontsize, dark_blue, thick, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "        # if torso_inclination >= 43:\n",
    "        #     detectedFrame = cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "        #     currentframenumber.append(detectedFrame-3)\n",
    "        # # Write frames.\n",
    "        # video_output.write(image)\n",
    "\n",
    "        # Write frames.\n",
    "        video_output.write(image)\n",
    "    except Exception as e:\n",
    "        # # print(\"Cannot Detect Anything\")\n",
    "        # # STR for timing\n",
    "        # currentFrame = cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "        # feetDist = \"-\"\n",
    "        # steps = \"-\"\n",
    "        # ball_release = \"-\"\n",
    "        # velo = \"-\"\n",
    "\n",
    "        # angle_text_string = 'Frame: '+str(currentFrame) +' Feet distance: '+ feetDist + ' Steps: '+ steps  + ' Release: '+ ball_release + ' Velocity '+ velo\n",
    "        # cv2.putText(image, angle_text_string, (10, text_y), font, fontsize, dark_blue, thick, cv2.LINE_AA)\n",
    "\n",
    "        print(e)\n",
    "        # Write frames.\n",
    "        video_output.write(image)\n",
    "print('Video is done!')\n",
    "cap.release()\n",
    "video_output.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-Reading Analysed Video for Screenshotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path ='./Analysedphoto'\n",
    "# isExist = os.path.exists(path)\n",
    "\n",
    "# if not isExist:\n",
    "#   # Create a new directory because it does not exist \n",
    "#   os.makedirs(path)\n",
    "#   print(\"Analysedphoto folder is created!\")\n",
    "\n",
    "# cap=cv2.VideoCapture('test_12-07-2022.mp4')\n",
    "# ret,frame= cap.read()\n",
    "# x=0\n",
    "# bool=True\n",
    "# testing=[74]\n",
    "# frameLen=int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "# while bool:\n",
    "#     for i in range(0,frameLen,1):\n",
    "#         if x>=len(testing):\n",
    "#             bool=False\n",
    "#             break\n",
    "#         # print(\"iv1 \", i)\n",
    "#         # print(x<len(currentframenumber))\n",
    "#         ret,frame= cap.read()\n",
    "#         if i== testing[x]:\n",
    "#             print(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "#             # print(\"iv2\", i)\n",
    "#             x=x+1\n",
    "#             print(x)\n",
    "#             cv2.imwrite(\"Analysedphoto/frame%d.jpg\" % i, frame)     # save frame as JPEG file      \n",
    "#             print('Read a new frame: ', ret)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
